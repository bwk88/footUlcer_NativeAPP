{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UvHN93y25FM",
        "outputId": "7beab965-c200-49e0-d5b2-01163f0459b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "\n",
        "# Mount the Google Drive to access the zip file\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set the path to the zip file in the mounted drive\n",
        "zip_path = '/content/drive/MyDrive/Dataset.zip'\n",
        "\n",
        "# Extract the zip file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xc820X57W1Gx"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get a batch of images from the training dataset\n",
        "x_batch, y_batch = next(train_datagen)\n",
        "\n",
        "# Plot the images\n",
        "fig, axes = plt.subplots(nrows=4, ncols=8, figsize=(12, 6))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(x_batch[i].reshape(224, 224), cmap='gray')\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.set_title(f'Class: {y_batch[i].argmax()}')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efzU1MM_6CD-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Concatenate, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def DFU_SPNet():\n",
        "    \"\"\"\n",
        "    Returns a DFU_SPNet model.\n",
        "\n",
        "    This model consists of two parallel convolutional layers that are stacked\n",
        "    together, with a Concatenate layer to merge the outputs of the parallel layers,\n",
        "    followed by a dense layer with dropout for classification.\n",
        "\n",
        "    Returns:\n",
        "        model (tensorflow.keras.models.Model): The DFU_SPNet model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the input layer\n",
        "    input_layer = Input(shape=(224, 224, 1))\n",
        "\n",
        "    # Define the first parallel convolutional layer\n",
        "    conv1a = Conv2D(32, (3, 3), activation='relu', padding='same')(input_layer)\n",
        "    conv1a = MaxPooling2D((2, 2))(conv1a)\n",
        "    conv1b = Conv2D(32, (5, 5), activation='relu', padding='same')(input_layer)\n",
        "    conv1b = MaxPooling2D((2, 2))(conv1b)\n",
        "\n",
        "    # Define the second parallel convolutional layer\n",
        "    conv2a = Conv2D(64, (3, 3), activation='relu', padding='same')(Concatenate()([conv1a, conv1b]))\n",
        "    conv2a = MaxPooling2D((2, 2))(conv2a)\n",
        "    conv2b = Conv2D(64, (5, 5), activation='relu', padding='same')(Concatenate()([conv1a, conv1b]))\n",
        "    conv2b = MaxPooling2D((2, 2))(conv2b)\n",
        "\n",
        "    # Merge the outputs of the parallel layers using Concatenate\n",
        "    merged = Concatenate()([conv2a, conv2b])\n",
        "\n",
        "    # Flatten the output of the merged layer\n",
        "    flatten = Flatten()(merged)\n",
        "\n",
        "    # Add a dense layer with dropout for classification\n",
        "    dense = Dense(128, activation='relu')(flatten)\n",
        "    dense = Dropout(0.5)(dense)\n",
        "    output_layer = Dense(2, activation='softmax')(dense)\n",
        "\n",
        "    # Define the model with input and output layers\n",
        "    model = Model(inputs=[input_layer], outputs=[output_layer])\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdYGm8ct6JRS"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Load the dataset using ImageDataGenerator with data augmentation\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                   rotation_range=10,\n",
        "                                   width_shift_range=0.1,\n",
        "                                   height_shift_range=0.1,\n",
        "                                   shear_range=0.1,\n",
        "                                   zoom_range=0.1,\n",
        "                                   horizontal_flip=True)\n",
        "train_dataset = train_datagen.flow_from_directory('/content/DFU Dataset/train',\n",
        "                                                  target_size=(224, 224),\n",
        "                                                  color_mode='grayscale',\n",
        "                                                  batch_size=32,\n",
        "                                                  class_mode='categorical')\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_dataset = test_datagen.flow_from_directory('/content/DFU Dataset/test',\n",
        "                                                target_size=(224, 224),\n",
        "                                                color_mode='grayscale',\n",
        "                                                batch_size=32,\n",
        "                                                class_mode='categorical')\n",
        "\n",
        "# Initialize the DFU_SPNet model\n",
        "model = DFU_SPNet()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_dataset,\n",
        "          epochs=50,\n",
        "          validation_data=test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGGCJ7i9Yh0F"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sirb4laYY4Ph"
      },
      "outputs": [],
      "source": [
        "history['accuracy']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dNo3dkSHZSwv"
      },
      "outputs": [],
      "source": [
        "# Evaluate model\n",
        "test_loss, test_acc = model.evaluate(test_dataset)\n",
        "print('Test loss:', test_loss)\n",
        "print('Test accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dcYxdy2ZHFs"
      },
      "outputs": [],
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sH7EDqCAaNoO"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXA2WkAYYwDr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMSRp0fWY0SV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7KnYE6YcnNQ"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "# Load an image for prediction\n",
        "img_path = '/content/DFU Dataset/test/1/2.01_110.jpg'\n",
        "img = image.load_img(img_path, target_size=(224, 224))\n",
        "img.show();\n",
        "img_arr = image.img_to_array(img)\n",
        "img_arr = np.expand_dims(img_arr, axis=0)\n",
        "img_arr /= 255.\n",
        "\n",
        "# Make prediction\n",
        "pred = model.predict(img_arr)\n",
        "\n",
        "# Print predicted class and probability\n",
        "if pred[0] < 0.5:\n",
        "    print('Non-ulcer foot image with probability', 1-pred[0])\n",
        "else:\n",
        "    print('Ulcer foot image with probability', pred[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tHvuKmOY1X3"
      },
      "outputs": [],
      "source": [
        "model_version = 1\n",
        "model.save('my_model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5HUDcimPnDN"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}